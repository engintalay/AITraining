# Finetune Environment Documentation

Bu dokÃ¼man, `../finetune` konumunda bulunan Python sanal ortamÄ±nÄ±n (virtual environment) nasÄ±l aktive edileceÄŸi, iÃ§eriÄŸi, sÄ±fÄ±rdan nasÄ±l oluÅŸturulacaÄŸÄ± ve **bu projenin nasÄ±l kullanÄ±lacaÄŸÄ±** hakkÄ±nda bilgiler iÃ§erir.

## Kurulum (EÄŸer environment yoksa)

EÄŸer `../finetune` klasÃ¶rÃ¼ mevcut deÄŸilse, aÅŸaÄŸÄ±daki komutlarla oluÅŸturun:


```bash
python3.12 -m venv ../finetune
source ../finetune/bin/activate
```

**DonanÄ±mÄ±nÄ±za uygun komutu seÃ§in:**

1.  **Ã–nce PyTorch'u kurun:**

    *   **NVIDIA (CUDA 12.1):**
        ```bash
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
        ```

    *   **AMD (ROCm 6.2.4):**
        ```bash
        pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/rocm6.2.4
        ```
    
    *   **CPU:**
        ```bash
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
        ```

2.  **DiÄŸer kÃ¼tÃ¼phaneleri yÃ¼kleyin:**

    ```bash
    pip install -r requirements.txt
    ```

*   **Vulkan:**
    *(Not: Standart PyTorch pip paketlerinde Vulkan desteÄŸi bulunmamaktadÄ±r. Kaynak kodundan derleme gerektirebilir.)*


(DetaylÄ± kÃ¼tÃ¼phane listesi iÃ§in dokÃ¼manÄ±n sonundaki [OrtamÄ± SÄ±fÄ±rdan Kurma](#ortamÄ±-sÄ±fÄ±rdan-kurma) bÃ¶lÃ¼mÃ¼ne bakabilirsiniz.)

## OrtamÄ± Aktive Etme

Mevcut `finetune` ortamÄ±nÄ± aktive etmek iÃ§in terminalde ÅŸu komutu Ã§alÄ±ÅŸtÄ±rÄ±n (AITraining klasÃ¶rÃ¼nde olduÄŸunuz varsayÄ±lmÄ±ÅŸtÄ±r):

```bash
source ../finetune/bin/activate
```

Ortamdan Ã§Ä±kmak iÃ§in:

```bash
deactivate
```

## Ã–nemli Not: AMD 780M / RDNA3 KullanÄ±cÄ±larÄ± Ä°Ã§in
EÄŸer **"HIP error: invalid device function"** hatasÄ± alÄ±rsanÄ±z, komutlarÄ±n baÅŸÄ±na ÅŸu ortam deÄŸiÅŸkenini ekleyerek Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
HSA_OVERRIDE_GFX_VERSION=11.0.0 python3 ...
```

Ã–rneÄŸin:
```bash
HSA_OVERRIDE_GFX_VERSION=11.0.0 python train.py
```

## Proje KullanÄ±mÄ±

Bu proje, **TinyLlama-1.1B** modelini `data.json` iÃ§erisindeki verilerle eÄŸitmek (finetune) ve test etmek iÃ§in geliÅŸtirilmiÅŸtir.

### 1. Modeli EÄŸitme (Finetuning)

Modeli `data.json` verisi ile eÄŸitmek iÃ§in ÅŸu komutu Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
python train.py
```

VarsayÄ±lan olarak `data.json` dosyasÄ±nÄ± kullanÄ±r. FarklÄ± bir veri dosyasÄ± kullanmak isterseniz:

```bash
python train.py benim_datam.json
```


Bu iÅŸlem tamamlandÄ±ÄŸÄ±nda, eÄŸitilmiÅŸ model (adapter) dosyalarÄ± `./out` klasÃ¶rÃ¼ne kaydedilecektir.
EÄŸitim sÃ¼resince `TrainingArguments` kullanÄ±ldÄ±ÄŸÄ± iÃ§in checkpointler de burada saklanÄ±r.

### 2. Modeli Test Etme (Inference)

EÄŸitilmiÅŸ modeli test etmek iÃ§in `test.py` dosyasÄ±nÄ± kullanÄ±n. Bu dosya `./out` klasÃ¶rÃ¼ndeki adapter'Ä± ve base modeli yÃ¼kler, bir soru sorar ve cevabÄ± ekrana basar.

```bash
python test.py
```

*Not: EÄŸer `out` klasÃ¶rÃ¼ yoksa veya boÅŸsa, Ã¶nce eÄŸitimi Ã§alÄ±ÅŸtÄ±rmanÄ±z gerekir.*

### 3. Base Model ile Test Etme

EÄŸitimden Ã¶nceki (ham) modelin nasÄ±l cevap verdiÄŸini gÃ¶rmek iÃ§in `test_base.py` dosyasÄ±nÄ± kullanabilirsiniz. Bu script, herhangi bir LoRA adapter kullanmadan saf TinyLlama modelini Ã§alÄ±ÅŸtÄ±rÄ±r.

```bash
python test_base.py
```

### 4. Veri Seti

`data.json` dosyasÄ±, eÄŸitim iÃ§in kullanÄ±lan soru-cevap Ã§iftlerini iÃ§erir. FormatÄ± ÅŸÃ¶yledir:

```json
[
    {
        "instruction": "Soru...",
        "response": "Cevap..."
    },
    ...
]
```

---

## EÄŸitim Kalitesini Ayarlama

EÄŸitim kalitesini ve performansÄ±nÄ± ayarlamak iÃ§in `train.py` dosyasÄ±ndaki aÅŸaÄŸÄ±daki parametreleri deÄŸiÅŸtirebilirsiniz:

### LoRA KonfigÃ¼rasyonu (lora_config)

```python
lora_config = LoraConfig(
    r=16,                    # LoRA rank (8-64 arasÄ±, yÃ¼ksek = daha fazla parametre)
    lora_alpha=32,           # LoRA alpha (genelde r*2, Ã¶ÄŸrenme hÄ±zÄ±nÄ± etkiler)
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # Hangi katmanlar eÄŸitilecek
    lora_dropout=0.05,       # Dropout oranÄ± (0.0-0.1, overfitting'i Ã¶nler)
    bias="none",             # Bias eÄŸitimi ("none", "all", "lora_only")
    task_type="CAUSAL_LM"
)
```

**Ã–neriler:**
- **Daha iyi kalite:** `r=32`, `lora_alpha=64` (daha yavaÅŸ, daha fazla bellek)
- **Daha hÄ±zlÄ± eÄŸitim:** `r=8`, `lora_alpha=16` (daha az parametre)
- **Overfitting varsa:** `lora_dropout=0.1` artÄ±rÄ±n

### EÄŸitim Parametreleri (SFTConfig)

```python
training_args = SFTConfig(
    per_device_train_batch_size=1,      # Batch size (1-4, GPU belleÄŸine gÃ¶re)
    gradient_accumulation_steps=16,     # Gradient biriktirme (efektif batch = batch_size * bu deÄŸer)
    num_train_epochs=30,                # Epoch sayÄ±sÄ± (10-50 arasÄ±)
    learning_rate=1e-4,                 # Ã–ÄŸrenme hÄ±zÄ± (1e-5 ile 5e-4 arasÄ±)
    max_grad_norm=0.3,                  # Gradient clipping (0.3-1.0 arasÄ±)
    logging_steps=1,                    # Her kaÃ§ adÄ±mda log basÄ±lacak
    dataloader_num_workers=2,           # Veri yÃ¼kleme thread sayÄ±sÄ±
)
```

**Ã–neriler:**
- **Daha iyi Ã¶ÄŸrenme:** `num_train_epochs=50`, `learning_rate=2e-4`
- **Daha hÄ±zlÄ± eÄŸitim:** `num_train_epochs=10`, batch_size artÄ±rÄ±n (GPU belleÄŸi yeterse)
- **KararsÄ±z eÄŸitim:** `learning_rate=5e-5` dÃ¼ÅŸÃ¼rÃ¼n, `max_grad_norm=0.5` artÄ±rÄ±n
- **Efektif batch size:** `batch_size * gradient_accumulation_steps` = 16-32 olmalÄ±

### Veri Kalitesi

- **Daha fazla veri:** Daha iyi genelleme
- **Ã‡eÅŸitli Ã¶rnekler:** FarklÄ± soru tipleri ekleyin
- **Temiz veri:** TutarsÄ±z veya hatalÄ± Ã¶rnekleri temizleyin
- **Dengeli daÄŸÄ±lÄ±m:** Her kategoriden benzer sayÄ±da Ã¶rnek

### Performans vs Kalite Dengesi

| Ayar | HÄ±z | Kalite | Bellek |
|------|-----|--------|--------|
| `r=8, epochs=10` | âš¡âš¡âš¡ | â­â­ | ğŸ’¾ |
| `r=16, epochs=30` | âš¡âš¡ | â­â­â­ | ğŸ’¾ğŸ’¾ |
| `r=32, epochs=50` | âš¡ | â­â­â­â­ | ğŸ’¾ğŸ’¾ğŸ’¾ |

---

## YÃ¼klÃ¼ KÃ¼tÃ¼phaneler

AÅŸaÄŸÄ±da ortamda yÃ¼klÃ¼ olan temel kÃ¼tÃ¼phaneler ve versiyonlarÄ± listelenmiÅŸtir:

| KÃ¼tÃ¼phane | Versiyon |
|-----------|----------|
| python | 3.12 |
| torch | 2.6.0+rocm6.2.4 |
| transformers | 4.57.3 |
| datasets | 4.4.2 |
| peft | 0.18.0 |
| trl | 0.26.2 |
| bitsandbytes | 0.49.0 |
| accelerate | 1.12.0 |
| huggingface-hub | 0.36.0 |

(Tam liste aÅŸaÄŸÄ±da `requirements.txt` bÃ¶lÃ¼mÃ¼nde mevcuttur.)

## OrtamÄ± SÄ±fÄ±rdan Kurma

Bu ortamÄ± sÄ±fÄ±rdan oluÅŸturmak isterseniz aÅŸaÄŸÄ±daki adÄ±mlarÄ± takip edebilirsiniz.

1. **Yeni bir sanal ortam oluÅŸturun:**

```bash
```bash
python3.12 -m venv finetune
source finetune/bin/activate
```

2. **Gerekli kÃ¼tÃ¼phaneleri yÃ¼kleyin:**

PyTorch (CUDA 12.1 destekli) ve diÄŸer temel yapay zeka kÃ¼tÃ¼phaneleri iÃ§in:

```bash
# Ã–nce pip'i gÃ¼ncelleyin
pip install --upgrade pip

# PyTorch Kurulumu (AMD ROCm 6.2.4)
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/rocm6.2.4

# DiÄŸer KÃ¼tÃ¼phaneler
pip install transformers==4.57.3 datasets==4.4.2 peft==0.18.0 trl==0.26.2 bitsandbytes==0.49.0 accelerate==1.12.0 huggingface-hub==0.36.0 python-dateutil==2.9.0.post0 pytz==2025.2 six==1.17.0
```

Alternatif olarak, aÅŸaÄŸÄ±daki iÃ§eriÄŸi `requirements.txt` dosyasÄ±na kaydedip tek komutla yÃ¼kleyebilirsiniz:

**requirements.txt iÃ§eriÄŸi:**

(GÃ¼ncel tam liste iÃ§in `requirements.txt` dosyasÄ±na bakÄ±nÄ±z.)

YÃ¼kleme komutu:

1.  **Ã–nce PyTorch'u kurun:**

    *   **NVIDIA (CUDA 12.1):**
        ```bash
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
        ```

    *   **AMD (ROCm 6.2.4):**
        ```bash
        pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/rocm6.2.4
        ```
    
    *   **CPU:**
        ```bash
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
        ```

2.  **DiÄŸer kÃ¼tÃ¼phaneleri yÃ¼kleyin:**

    ```bash
    pip install -r requirements.txt
    ```




